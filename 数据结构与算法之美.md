# 数据结构与算法之美

### 开篇词 | 从今天起，跨过“数据结构与算法”这道坎

### 01 | 为什么要学习数据结构和算法？

### 02 | 如何抓住重点，系统高效地学习数据结构与算法？

***什么是数据结构？什么是算法？***

从广义上讲，数据结构就是一组数据的存储结构，算法就是操作数据的一组方法。

从狭义上讲，也就是我们专栏要讲的，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。

数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。 因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。

***学习的重点在什么地方？***

数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无招胜有招！

在学习数据结构和算法的过程中，你也要注意，不要只是死记硬背，不要为了学习而学习，而是要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”。

学习数据结构和算法的过程，是非常好的思维训练的过程，所以，千万不要被动地记忆，要多辩证地思考，多问为什么。

***一些可以让你事半功倍的学习技巧***

边学边练，适度刷题

多问、多思考、多互动

打怪升级学习法

知识需要沉淀，不要想试图一下子掌握所有

### 03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？

***为什么需要复杂度分析？***

**事后统计法**

我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。

测试结果非常依赖测试环境

测试结果受数据规模的影响很大

我们需要一个不用具体的测试数据，就可以粗略地估计算法的执行效率的方法。这就是我们今天要讲的时间、空间复杂度分析方法。

***大 O 复杂度表示法***

所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。

![大 O 复杂度表示法](大 O 复杂度表示法.jpg)

T(n) 我们已经讲过了，它表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行次数的总和。因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比。

大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。

而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略，我们只需要记录一个最大量级就可以了。

***时间复杂度分析***

只关注循环执行次数最多的那一段代码

加法法则：总复杂度等于量级最大的那段代码的复杂度

乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

***几种常见时间复杂度实例分析***

![复杂度量级](复杂度量级.jpg)

对于刚罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O($2^n$) 和 O(n!)。

当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。

**O(1)**

只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

**O(logn)、O(nlogn)**

```java
i = 1;
while (i <= n) {
  i = i * 2;
}
```

实际上，变量 i 的取值就是一个等比数列。

![等比数列](等比数列.jpg)

所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。

x=$log_2 n$，所以，这段代码的时间复杂度就是 O($log_2 n$)。

实际上，不管是以 2 为底，还是以 3 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。

对数之间是可以互相转换的，$log_3 n$ 就等于 $log_3 2$ * $log_2 n$，所以 O($log_3 n$) = O(C * $log_2 n$)，其中 C=$log_3 2$ 是一个常量。

**O(m+n)、O(m*n)**

代码的复杂度由两个数据的规模来决定。

***空间复杂度分析***

空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。

我们常见的空间复杂度就是 O(1)、O(n)、O($n^2$ )，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。

***复杂度图表***

![复杂度图表](复杂度图表.jpg)

### 04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度

***最好、最坏情况时间复杂度***

最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。就像我们刚刚讲到的，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。

最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。就像刚举的那个例子，如果数组中没有要查找的变量 x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。

***平均情况时间复杂度***

要查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0～n-1 位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即：

![平均时间复杂度](平均时间复杂度_1.jpg)

我们刚讲的这 n+1 种情况，出现的概率并不是一样的。

我们知道，要查找的变量 x，要么在数组里，要么不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。

如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样：

![平均时间复杂度](平均时间复杂度_2.jpg)

这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。

***均摊时间复杂度***

对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。

### 05 | 数组：为什么很多编程语言中数组都从 0 开始编号？

***如何实现随机访问？***

数组（Array）是一种**线性表**数据结构。它用一组**连续的内存空间**，来存储一组具有**相同类型的数据**。

计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：

```c
a[i]_address = base_address + i * data_type_size
```

其中 data_type_size 表示数组中每个元素的大小。

***低效的“插入”和“删除”***

**插入操作**

如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。

利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。

**删除操作**

在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。

每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。

***警惕数组的访问越界问题***

数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。

***容器能否完全替代数组？***

Java ArrayList 最大的优势就是可以将很多数组操作的细节封装起来（内存申请和数据搬移操作）。另外，它还有一个优势，就是支持动态扩容。

Java ArrayList 无法存储基本类型，比如 int、long 需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。

如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。

当要表示多维数组时，用数组往往会更加直观。

***解答开篇***

从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式：

```c
a[k]_address = base_address + k * type_size
```

但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为：

```c
a[k]_address = base_address + (k-1) * type_size
```

从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。

C 语言设计者用 0 开始计数数组下标，之后的 Java、JavaScript 等高级语言都效仿了 C 语言，或者说，为了在一定程度上减少 C 语言程序员学习 Java 的学习成本，因此继续沿用了从 0 开始计数的习惯。